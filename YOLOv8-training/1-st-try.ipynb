{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aea6550b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3070 Ti\n",
      "CUDA memory: 8.0 GB\n",
      "Downloaded YOLOv8n base model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# !pip install ultralytics==8.0.238 opencv-python-headless==4.8.1.78\n",
    "# !pip install torch==1.10.0 torchvision==0.11.0 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "# !pip install onnx onnxruntime tensorrt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.engine.model import Model\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from ultralytics.utils.torch_utils import select_device\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "# Check environment\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Download official YOLOv8n model (smallest, best for Jetson)\n",
    "!wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt\n",
    "print(\"Downloaded YOLOv8n base model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a49bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv8GroundRobotDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified YOLOv8 for ground-level robot detection with orientation.\n",
    "    Handles arbitrary number of robots (0 to N) in each frame.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path='yolov8n.pt', num_classes=2):  # robot and goal_box\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained YOLOv8\n",
    "        self.base_model = YOLO(model_path)\n",
    "        self.model = self.base_model.model\n",
    "        \n",
    "        # Modify the detection head to include orientation\n",
    "        # YOLOv8n has 3 detection heads for different scales\n",
    "        # We'll modify each to output: [x, y, w, h, obj, cls1, cls2, sin(yaw), cos(yaw)]\n",
    "        \n",
    "        # Get the Detect module\n",
    "        self.detect = self.model.model[-1]\n",
    "        \n",
    "        # Original YOLOv8n outputs: nc=80 classes -> 85 channels per anchor\n",
    "        # Our output: 2 classes + 2 orientation values -> 9 channels per anchor\n",
    "        # (4 bbox + 1 obj + 2 cls + 2 orientation)\n",
    "        \n",
    "        old_detect = self.detect\n",
    "        in_channels = [ch.out_channels for ch in self.model.model[-2].m]\n",
    "        \n",
    "        # Create new detection heads with orientation output\n",
    "        self.detect.cv3 = nn.ModuleList(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(x, 256, 3, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(256, (4 + 1 + num_classes + 2) * old_detect.na, 1)\n",
    "            ) for x in in_channels\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.detect.cv3:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class OrientationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom loss for orientation estimation.\n",
    "    Uses sin/cos representation to handle angle discontinuity.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, pred_sin_cos, target_angle, mask):\n",
    "        \"\"\"\n",
    "        pred_sin_cos: [batch, 2] predicted sin and cos values\n",
    "        target_angle: [batch] ground truth angles in radians\n",
    "        mask: [batch] binary mask for valid orientations (robots only)\n",
    "        \"\"\"\n",
    "        if mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=pred_sin_cos.device)\n",
    "        \n",
    "        # Normalize predictions to unit circle\n",
    "        pred_norm = torch.nn.functional.normalize(pred_sin_cos[mask], p=2, dim=1)\n",
    "        \n",
    "        # Convert target angles to sin/cos\n",
    "        target_sin = torch.sin(target_angle[mask]).unsqueeze(1)\n",
    "        target_cos = torch.cos(target_angle[mask]).unsqueeze(1)\n",
    "        target_sin_cos = torch.cat([target_sin, target_cos], dim=1)\n",
    "        \n",
    "        # L2 loss in sin/cos space\n",
    "        loss = torch.nn.functional.mse_loss(pred_norm, target_sin_cos)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4328b807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics:\n",
      "Total images: 30000\n",
      "Robot count distribution:\n",
      "  0_robots: 30000 images (100.0%)\n",
      "\n",
      "Processing 30000 valid images...\n",
      "\n",
      "Processing train: 21000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|██████████| 21000/21000 [02:03<00:00, 169.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing val: 6000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val: 100%|██████████| 6000/6000 [00:48<00:00, 123.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test: 3000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|██████████| 3000/3000 [00:25<00:00, 119.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset prepared at: jetbot_groundview_dataset\n",
      "Configuration: jetbot_groundview_dataset\\data.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class GroundViewDatasetPrep:\n",
    "    \"\"\"\n",
    "    Prepare dataset specifically for ground-level robot detection.\n",
    "    Handles variable number of robots per image.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, source_dir, output_dir):\n",
    "        self.source_dir = Path(source_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        \n",
    "        # Create directory structure\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            (self.output_dir / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "            (self.output_dir / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def analyze_dataset_statistics(self):\n",
    "        \"\"\"Analyze robot count distribution and spatial patterns\"\"\"\n",
    "        stats = defaultdict(int)\n",
    "        robot_counts = []\n",
    "        \n",
    "        for json_path in self.source_dir.glob(\"screenshot_*.json\"):\n",
    "            if '_debug' in json_path.name:\n",
    "                continue\n",
    "                \n",
    "            with open(json_path) as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            # Handle both old and new JSON format\n",
    "            if isinstance(data, dict) and 'pose_data' in data:\n",
    "                pose_list = data['pose_data']\n",
    "            else:\n",
    "                pose_list = data\n",
    "                \n",
    "            robot_count = sum(1 for p in pose_list if 'robot' in p.get('robot_id', ''))\n",
    "            robot_counts.append(robot_count)\n",
    "            stats[f'{robot_count}_robots'] += 1\n",
    "        \n",
    "        print(\"Dataset Statistics:\")\n",
    "        print(f\"Total images: {len(robot_counts)}\")\n",
    "        print(f\"Robot count distribution:\")\n",
    "        for k, v in sorted(stats.items()):\n",
    "            print(f\"  {k}: {v} images ({v/len(robot_counts)*100:.1f}%)\")\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def process_dataset(self):\n",
    "        \"\"\"Process images with enhanced ground-view awareness\"\"\"\n",
    "        \n",
    "        # First analyze the dataset\n",
    "        stats = self.analyze_dataset_statistics()\n",
    "        \n",
    "        # Get all valid images\n",
    "        all_images = []\n",
    "        for img_path in self.source_dir.glob(\"screenshot_*.png\"):\n",
    "            if '_debug' not in img_path.name:\n",
    "                txt_path = img_path.with_suffix('.txt')\n",
    "                json_path = img_path.with_suffix('.json')\n",
    "                if txt_path.exists() and json_path.exists():\n",
    "                    all_images.append(img_path)\n",
    "        \n",
    "        print(f\"\\nProcessing {len(all_images)} valid images...\")\n",
    "        \n",
    "        # Split dataset (70/20/10)\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(all_images)\n",
    "        \n",
    "        train_size = int(0.7 * len(all_images))\n",
    "        val_size = int(0.2 * len(all_images))\n",
    "        \n",
    "        train_images = all_images[:train_size]\n",
    "        val_images = all_images[train_size:train_size + val_size]\n",
    "        test_images = all_images[train_size + val_size:]\n",
    "        \n",
    "        # Process each split\n",
    "        for split, images in [('train', train_images), ('val', val_images), ('test', test_images)]:\n",
    "            print(f\"\\nProcessing {split}: {len(images)} images\")\n",
    "            self._process_split(images, split)\n",
    "        \n",
    "        # Create data.yaml\n",
    "        self._create_data_yaml()\n",
    "    \n",
    "    def _process_split(self, image_paths, split):\n",
    "        \"\"\"Process images for a specific split\"\"\"\n",
    "        \n",
    "        for img_path in tqdm(image_paths, desc=f\"Processing {split}\"):\n",
    "            # Copy image\n",
    "            dest_img = self.output_dir / split / 'images' / img_path.name\n",
    "            shutil.copy(img_path, dest_img)\n",
    "            \n",
    "            # Process labels\n",
    "            txt_path = img_path.with_suffix('.txt')\n",
    "            json_path = img_path.with_suffix('.json')\n",
    "            \n",
    "            with open(json_path) as f:\n",
    "                json_data = json.load(f)\n",
    "            \n",
    "            # Handle JSON format\n",
    "            if isinstance(json_data, dict) and 'pose_data' in json_data:\n",
    "                pose_list = json_data['pose_data']\n",
    "            else:\n",
    "                pose_list = json_data\n",
    "            \n",
    "            # Read YOLO labels\n",
    "            with open(txt_path) as f:\n",
    "                yolo_lines = f.readlines()\n",
    "            \n",
    "            # Create enhanced labels\n",
    "            new_labels = []\n",
    "            \n",
    "            for line in yolo_lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    old_class = int(parts[0])\n",
    "                    bbox = parts[1:5]\n",
    "                    \n",
    "                    # Remap classes: 0,1,2 -> 0 (robot), 3 -> 1 (goal_box)\n",
    "                    if old_class < 3:  # Robot\n",
    "                        new_class = 0\n",
    "                        \n",
    "                        # Find orientation from JSON\n",
    "                        robot_id = f\"target_bot_{old_class + 1}\"\n",
    "                        yaw = 0.0\n",
    "                        \n",
    "                        for pose in pose_list:\n",
    "                            if pose.get('robot_id') == robot_id:\n",
    "                                yaw = pose.get('yaw', 0.0)\n",
    "                                break\n",
    "                        \n",
    "                        # Store sin and cos instead of angle (better for training)\n",
    "                        sin_yaw = np.sin(yaw)\n",
    "                        cos_yaw = np.cos(yaw)\n",
    "                        \n",
    "                        # Format: class x y w h sin(yaw) cos(yaw)\n",
    "                        new_labels.append(f\"{new_class} {' '.join(bbox)} {sin_yaw:.6f} {cos_yaw:.6f}\")\n",
    "                    \n",
    "                    else:  # Goal box\n",
    "                        new_class = 1\n",
    "                        # Goal box has no orientation\n",
    "                        new_labels.append(f\"{new_class} {' '.join(bbox)} 0.0 0.0\")\n",
    "            \n",
    "            # Write new labels\n",
    "            dest_label = self.output_dir / split / 'labels' / txt_path.name\n",
    "            with open(dest_label, 'w') as f:\n",
    "                f.write('\\n'.join(new_labels))\n",
    "    \n",
    "    def _create_data_yaml(self):\n",
    "        \"\"\"Create YOLOv8 data configuration\"\"\"\n",
    "        \n",
    "        data_config = {\n",
    "            'path': str(self.output_dir.absolute()),\n",
    "            'train': 'train/images',\n",
    "            'val': 'val/images', \n",
    "            'test': 'test/images',\n",
    "            \n",
    "            # Classes\n",
    "            'names': {\n",
    "                0: 'robot',  # Any jetbot\n",
    "                1: 'goal_box'\n",
    "            },\n",
    "            'nc': 2,\n",
    "            \n",
    "            # Custom fields for our model\n",
    "            'orientation': {\n",
    "                'enabled': True,\n",
    "                'classes': [0],  # Only robots have orientation\n",
    "                'representation': 'sin_cos'  # Using sin/cos representation\n",
    "            },\n",
    "            \n",
    "            # Ground-view specific settings\n",
    "            'perspective': 'ground',\n",
    "            'camera_height': 0.1,  # 10cm off ground\n",
    "            'fov': 160  # Wide angle camera on jetbot\n",
    "        }\n",
    "        \n",
    "        yaml_path = self.output_dir / 'data.yaml'\n",
    "        with open(yaml_path, 'w') as f:\n",
    "            yaml.dump(data_config, f, default_flow_style=False)\n",
    "        \n",
    "        print(f\"\\nDataset prepared at: {self.output_dir}\")\n",
    "        print(f\"Configuration: {yaml_path}\")\n",
    "\n",
    "# Prepare the dataset\n",
    "dataset_prep = GroundViewDatasetPrep(\n",
    "    source_dir=\"C:/temp\",  # Your synthetic data\n",
    "    output_dir=\"jetbot_groundview_dataset\"\n",
    ")\n",
    "dataset_prep.process_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f39e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing custom YOLO wrapper...\n",
      "\n",
      "Processing screenshot_00000.png...\n",
      "  Error: 'Detect' object has no attribute 'na'\n",
      "\n",
      "Processing screenshot_00005.png...\n",
      "  Error: 'Detect' object has no attribute 'na'\n",
      "\n",
      "Processing screenshot_00010.png...\n",
      "  Error: 'Detect' object has no attribute 'na'\n",
      "\n",
      "Processing screenshot_00015.png...\n",
      "  Error: 'Detect' object has no attribute 'na'\n",
      "\n",
      "Processing screenshot_00020.png...\n",
      "  Error: 'Detect' object has no attribute 'na'\n",
      "\n",
      "Processing screenshot_00025.png...\n",
      "  Error: 'Detect' object has no attribute 'na'\n",
      "\n",
      "Processing screenshot_00030.png...\n",
      "  Error: 'Detect' object has no attribute 'na'\n",
      "\n",
      "Processing screenshot_00035.png...\n",
      "  Error: 'Detect' object has no attribute 'na'\n",
      "\n",
      "Processing screenshot_00040.png...\n",
      "  Error: 'Detect' object has no attribute 'na'\n",
      "\n",
      "Processing screenshot_00045.png...\n",
      "  Error: 'Detect' object has no attribute 'na'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 212, in test_wrapper\n",
      "    wrapper.visualize(img_path, output_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 148, in visualize\n",
      "    results = self.predict_with_orientation(image_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 80, in predict_with_orientation\n",
      "    results = self.yolo.predict(image_path, conf=conf, iou=iou, verbose=False)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\model.py\", line 555, in predict\n",
      "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 218, in __call__\n",
      "    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 36, in generator_context\n",
      "    response = gen.send(None)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 299, in stream_inference\n",
      "    self.model.warmup(\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 808, in warmup\n",
      "    self.forward(im)  # warmup\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 592, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize, embed=embed, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 115, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 133, in predict\n",
      "    return self._predict_once(x, profile, visualize, embed)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 156, in _predict_once\n",
      "    x = m(x)  # run\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 50, in patched_forward\n",
      "    na = detect.na  # number of anchors\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1931, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'Detect' object has no attribute 'na'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 212, in test_wrapper\n",
      "    wrapper.visualize(img_path, output_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 148, in visualize\n",
      "    results = self.predict_with_orientation(image_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 80, in predict_with_orientation\n",
      "    results = self.yolo.predict(image_path, conf=conf, iou=iou, verbose=False)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\model.py\", line 555, in predict\n",
      "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 218, in __call__\n",
      "    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 36, in generator_context\n",
      "    response = gen.send(None)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 299, in stream_inference\n",
      "    self.model.warmup(\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 808, in warmup\n",
      "    self.forward(im)  # warmup\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 592, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize, embed=embed, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 115, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 133, in predict\n",
      "    return self._predict_once(x, profile, visualize, embed)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 156, in _predict_once\n",
      "    x = m(x)  # run\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 50, in patched_forward\n",
      "    na = detect.na  # number of anchors\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1931, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'Detect' object has no attribute 'na'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 212, in test_wrapper\n",
      "    wrapper.visualize(img_path, output_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 148, in visualize\n",
      "    results = self.predict_with_orientation(image_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 80, in predict_with_orientation\n",
      "    results = self.yolo.predict(image_path, conf=conf, iou=iou, verbose=False)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\model.py\", line 555, in predict\n",
      "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 218, in __call__\n",
      "    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 36, in generator_context\n",
      "    response = gen.send(None)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 299, in stream_inference\n",
      "    self.model.warmup(\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 808, in warmup\n",
      "    self.forward(im)  # warmup\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 592, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize, embed=embed, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 115, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 133, in predict\n",
      "    return self._predict_once(x, profile, visualize, embed)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 156, in _predict_once\n",
      "    x = m(x)  # run\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 50, in patched_forward\n",
      "    na = detect.na  # number of anchors\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1931, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'Detect' object has no attribute 'na'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 212, in test_wrapper\n",
      "    wrapper.visualize(img_path, output_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 148, in visualize\n",
      "    results = self.predict_with_orientation(image_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 80, in predict_with_orientation\n",
      "    results = self.yolo.predict(image_path, conf=conf, iou=iou, verbose=False)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\model.py\", line 555, in predict\n",
      "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 218, in __call__\n",
      "    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 36, in generator_context\n",
      "    response = gen.send(None)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 299, in stream_inference\n",
      "    self.model.warmup(\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 808, in warmup\n",
      "    self.forward(im)  # warmup\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 592, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize, embed=embed, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 115, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 133, in predict\n",
      "    return self._predict_once(x, profile, visualize, embed)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 156, in _predict_once\n",
      "    x = m(x)  # run\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 50, in patched_forward\n",
      "    na = detect.na  # number of anchors\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1931, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'Detect' object has no attribute 'na'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 212, in test_wrapper\n",
      "    wrapper.visualize(img_path, output_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 148, in visualize\n",
      "    results = self.predict_with_orientation(image_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 80, in predict_with_orientation\n",
      "    results = self.yolo.predict(image_path, conf=conf, iou=iou, verbose=False)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\model.py\", line 555, in predict\n",
      "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 218, in __call__\n",
      "    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 36, in generator_context\n",
      "    response = gen.send(None)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 299, in stream_inference\n",
      "    self.model.warmup(\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 808, in warmup\n",
      "    self.forward(im)  # warmup\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 592, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize, embed=embed, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 115, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 133, in predict\n",
      "    return self._predict_once(x, profile, visualize, embed)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 156, in _predict_once\n",
      "    x = m(x)  # run\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 50, in patched_forward\n",
      "    na = detect.na  # number of anchors\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1931, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'Detect' object has no attribute 'na'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 212, in test_wrapper\n",
      "    wrapper.visualize(img_path, output_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 148, in visualize\n",
      "    results = self.predict_with_orientation(image_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 80, in predict_with_orientation\n",
      "    results = self.yolo.predict(image_path, conf=conf, iou=iou, verbose=False)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\model.py\", line 555, in predict\n",
      "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 218, in __call__\n",
      "    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 36, in generator_context\n",
      "    response = gen.send(None)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 299, in stream_inference\n",
      "    self.model.warmup(\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 808, in warmup\n",
      "    self.forward(im)  # warmup\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 592, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize, embed=embed, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 115, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 133, in predict\n",
      "    return self._predict_once(x, profile, visualize, embed)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 156, in _predict_once\n",
      "    x = m(x)  # run\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 50, in patched_forward\n",
      "    na = detect.na  # number of anchors\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1931, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'Detect' object has no attribute 'na'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 212, in test_wrapper\n",
      "    wrapper.visualize(img_path, output_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 148, in visualize\n",
      "    results = self.predict_with_orientation(image_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 80, in predict_with_orientation\n",
      "    results = self.yolo.predict(image_path, conf=conf, iou=iou, verbose=False)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\model.py\", line 555, in predict\n",
      "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 218, in __call__\n",
      "    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 36, in generator_context\n",
      "    response = gen.send(None)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 299, in stream_inference\n",
      "    self.model.warmup(\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 808, in warmup\n",
      "    self.forward(im)  # warmup\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 592, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize, embed=embed, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 115, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 133, in predict\n",
      "    return self._predict_once(x, profile, visualize, embed)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 156, in _predict_once\n",
      "    x = m(x)  # run\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 50, in patched_forward\n",
      "    na = detect.na  # number of anchors\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1931, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'Detect' object has no attribute 'na'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 212, in test_wrapper\n",
      "    wrapper.visualize(img_path, output_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 148, in visualize\n",
      "    results = self.predict_with_orientation(image_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 80, in predict_with_orientation\n",
      "    results = self.yolo.predict(image_path, conf=conf, iou=iou, verbose=False)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\model.py\", line 555, in predict\n",
      "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 218, in __call__\n",
      "    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 36, in generator_context\n",
      "    response = gen.send(None)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 299, in stream_inference\n",
      "    self.model.warmup(\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 808, in warmup\n",
      "    self.forward(im)  # warmup\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 592, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize, embed=embed, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 115, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 133, in predict\n",
      "    return self._predict_once(x, profile, visualize, embed)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 156, in _predict_once\n",
      "    x = m(x)  # run\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 50, in patched_forward\n",
      "    na = detect.na  # number of anchors\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1931, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'Detect' object has no attribute 'na'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 212, in test_wrapper\n",
      "    wrapper.visualize(img_path, output_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 148, in visualize\n",
      "    results = self.predict_with_orientation(image_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 80, in predict_with_orientation\n",
      "    results = self.yolo.predict(image_path, conf=conf, iou=iou, verbose=False)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\model.py\", line 555, in predict\n",
      "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 218, in __call__\n",
      "    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 36, in generator_context\n",
      "    response = gen.send(None)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 299, in stream_inference\n",
      "    self.model.warmup(\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 808, in warmup\n",
      "    self.forward(im)  # warmup\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 592, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize, embed=embed, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 115, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 133, in predict\n",
      "    return self._predict_once(x, profile, visualize, embed)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 156, in _predict_once\n",
      "    x = m(x)  # run\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 50, in patched_forward\n",
      "    na = detect.na  # number of anchors\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1931, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'Detect' object has no attribute 'na'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 212, in test_wrapper\n",
      "    wrapper.visualize(img_path, output_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 148, in visualize\n",
      "    results = self.predict_with_orientation(image_path)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 80, in predict_with_orientation\n",
      "    results = self.yolo.predict(image_path, conf=conf, iou=iou, verbose=False)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\model.py\", line 555, in predict\n",
      "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 218, in __call__\n",
      "    return list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 36, in generator_context\n",
      "    response = gen.send(None)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 299, in stream_inference\n",
      "    self.model.warmup(\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 808, in warmup\n",
      "    self.forward(im)  # warmup\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\", line 592, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize, embed=embed, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 115, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 133, in predict\n",
      "    return self._predict_once(x, profile, visualize, embed)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 156, in _predict_once\n",
      "    x = m(x)  # run\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\fredr\\AppData\\Local\\Temp\\ipykernel_28768\\1303497928.py\", line 50, in patched_forward\n",
      "    na = detect.na  # number of anchors\n",
      "  File \"c:\\Users\\fredr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1931, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'Detect' object has no attribute 'na'\n"
     ]
    }
   ],
   "source": [
    "class JetsonOptimizedTrainer:\n",
    "    \"\"\"\n",
    "    Training pipeline optimized for Jetson Nano deployment.\n",
    "    Uses mixed precision, model pruning, and quantization-aware training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_yaml, base_model='yolov8n.pt'):\n",
    "        self.data_yaml = data_yaml\n",
    "        self.base_model = base_model\n",
    "        self.device = select_device('0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def create_training_args(self):\n",
    "        \"\"\"Create training arguments optimized for ground-view detection\"\"\"\n",
    "        \n",
    "        args = {\n",
    "            'model': self.base_model,\n",
    "            'data': self.data_yaml,\n",
    "            'epochs': 300,\n",
    "            'patience': 50,\n",
    "            'batch': 16,  # Adjust based on GPU memory\n",
    "            'imgsz': 512,  # Smaller than 640 for faster inference on Jetson\n",
    "            'device': self.device,\n",
    "            \n",
    "            # Optimization settings\n",
    "            'optimizer': 'AdamW',\n",
    "            'lr0': 0.01,\n",
    "            'lrf': 0.01,\n",
    "            'momentum': 0.937,\n",
    "            'weight_decay': 0.0005,\n",
    "            'warmup_epochs': 3.0,\n",
    "            'warmup_momentum': 0.8,\n",
    "            'warmup_bias_lr': 0.1,\n",
    "            \n",
    "            # Augmentation for ground-view\n",
    "            'hsv_h': 0.015,  # Hue variation\n",
    "            'hsv_s': 0.7,    # Saturation variation  \n",
    "            'hsv_v': 0.4,    # Value variation\n",
    "            'degrees': 0.0,   # No rotation (preserve ground plane)\n",
    "            'translate': 0.2, # Translation\n",
    "            'scale': 0.5,     # Scale variation (robots at different distances)\n",
    "            'shear': 0.0,     # No shear\n",
    "            'perspective': 0.0001,  # Slight perspective (ground view variation)\n",
    "            'flipud': 0.0,    # No vertical flip\n",
    "            'fliplr': 0.0,    # No horizontal flip (preserve left/right)\n",
    "            'mosaic': 0.8,    # Mosaic augmentation\n",
    "            'mixup': 0.1,     # Mixup augmentation\n",
    "            \n",
    "            # Loss weights\n",
    "            'box': 7.5,\n",
    "            'cls': 0.5,\n",
    "            'dfl': 1.5,\n",
    "            \n",
    "            # Model optimization\n",
    "            'half': True,  # FP16 training (faster, less memory)\n",
    "            'cos_lr': True,  # Cosine LR scheduler\n",
    "            \n",
    "            # Save settings\n",
    "            'save': True,\n",
    "            'save_period': 20,\n",
    "            'project': 'runs/train',\n",
    "            'name': 'jetbot_groundview',\n",
    "            'exist_ok': True,\n",
    "            'pretrained': True,\n",
    "            'plots': True,\n",
    "            \n",
    "            # Early stopping on Jetson metrics\n",
    "            'val': True,\n",
    "            'amp': True,  # Automatic mixed precision\n",
    "        }\n",
    "        \n",
    "        return args\n",
    "    \n",
    "    def train_with_orientation(self):\n",
    "        \"\"\"Train model with custom orientation loss\"\"\"\n",
    "        \n",
    "        # Initialize model with modifications\n",
    "        model = YOLO(self.base_model)\n",
    "        \n",
    "        # Get training args\n",
    "        args = self.create_training_args()\n",
    "        \n",
    "        # Add custom callbacks for orientation loss\n",
    "        def on_train_batch_end(trainer):\n",
    "            # Log orientation loss if available\n",
    "            pass\n",
    "        \n",
    "        # Train the model\n",
    "        results = model.train(**args)\n",
    "        \n",
    "        return model, results\n",
    "    \n",
    "    def optimize_for_jetson(self, model_path):\n",
    "        \"\"\"Post-training optimization for Jetson deployment\"\"\"\n",
    "        \n",
    "        model = YOLO(model_path)\n",
    "        \n",
    "        # 1. Export to ONNX with optimization\n",
    "        model.export(\n",
    "            format='onnx',\n",
    "            imgsz=512,\n",
    "            half=True,  # FP16\n",
    "            dynamic=False,  # Static shapes for TensorRT\n",
    "            simplify=True,\n",
    "            opset=12,\n",
    "            batch=1  # Single image inference\n",
    "        )\n",
    "        \n",
    "        # 2. Export to TensorRT for Jetson\n",
    "        model.export(\n",
    "            format='engine',\n",
    "            imgsz=512,\n",
    "            half=True,\n",
    "            device=0,\n",
    "            workspace=4,  # 4GB workspace\n",
    "            batch=1\n",
    "        )\n",
    "        \n",
    "        print(\"Model optimized for Jetson deployment\")\n",
    "\n",
    "# Start training\n",
    "trainer = JetsonOptimizedTrainer('jetbot_groundview_dataset/data.yaml')\n",
    "model, results = trainer.train_with_orientation()\n",
    "\n",
    "# Optimize for deployment\n",
    "trainer.optimize_for_jetson('runs/train/jetbot_groundview/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8b382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b4d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cca596ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: onnx 1.17.0\n",
      "Uninstalling onnx-1.17.0:\n",
      "  Successfully uninstalled onnx-1.17.0\n",
      "Collecting onnx==1.14.1\n",
      "  Downloading onnx-1.14.1-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\fredr\\anaconda3\\lib\\site-packages (from onnx==1.14.1) (1.25.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\fredr\\anaconda3\\lib\\site-packages (from onnx==1.14.1) (5.29.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\fredr\\anaconda3\\lib\\site-packages (from onnx==1.14.1) (4.12.2)\n",
      "Downloading onnx-1.14.1-cp39-cp39-win_amd64.whl (13.3 MB)\n",
      "   ---------------------------------------- 0.0/13.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.3 MB 1.4 MB/s eta 0:00:10\n",
      "    --------------------------------------- 0.3/13.3 MB 3.2 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.9/13.3 MB 7.2 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.0/13.3 MB 7.4 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.6/13.3 MB 7.1 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.1/13.3 MB 7.8 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.5/13.3 MB 7.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.1/13.3 MB 8.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.1/13.3 MB 8.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.4/13.3 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.0/13.3 MB 8.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.5/13.3 MB 8.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.1/13.3 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.4/13.3 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 6.1/13.3 MB 8.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.5/13.3 MB 8.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.1/13.3 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.5/13.3 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 8.2/13.3 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.5/13.3 MB 9.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 9.1/13.3 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.6/13.3 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 10.2/13.3 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.5/13.3 MB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.1/13.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.5/13.3 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.0/13.3 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.6/13.3 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.6/13.3 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.2/13.3 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.3/13.3 MB 9.8 MB/s eta 0:00:00\n",
      "Installing collected packages: onnx\n",
      "Successfully installed onnx-1.14.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: uiutil 1.38.0 has a non-standard dependency specifier future>=0.16.0attrs>=16.3.0. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of uiutil or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall onnx -y\n",
    "! pip install onnx==1.14.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63496df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.141  Python-3.9.18 torch-2.5.1+cu121 CPU (AMD Ryzen 9 7950X 16-Core Processor)\n",
      "WARNING half=True only compatible with GPU export, i.e. use device=0\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs\\train\\jetbot_groundview\\weights\\best.pt' with input shape (1, 3, 416, 416) BCHW and output shape(s) (1, 6, 3549) (6.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.53...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  0.5s, saved as 'runs\\train\\jetbot_groundview\\weights\\best.onnx' (11.6 MB)\n",
      "\n",
      "Export complete (0.6s)\n",
      "Results saved to \u001b[1mC:\\Users\\fredr\\BTH\\Robotik Project\\Robotics\\YOLOv8-training\\runs\\train\\jetbot_groundview\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=runs\\train\\jetbot_groundview\\weights\\best.onnx imgsz=416  \n",
      "Validate:        yolo val task=detect model=runs\\train\\jetbot_groundview\\weights\\best.onnx imgsz=416 data=jetbot_groundview_dataset/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.141  Python-3.9.18 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3070 Ti, 8192MiB)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs\\train\\jetbot_groundview\\weights\\best.pt' with input shape (1, 3, 416, 416) BCHW and output shape(s) (1, 6, 3549) (6.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.53...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  0.9s, saved as 'runs\\train\\jetbot_groundview\\weights\\best.onnx' (11.6 MB)\n",
      "ERROR \u001b[34m\u001b[1mTensorRT:\u001b[0m export failure 0.9s: No module named 'tensorrt'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorrt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\exporter.py:884\u001b[0m, in \u001b[0;36mExporter.export_engine\u001b[1;34m(self, dla, prefix)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 884\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorrt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtrt\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorrt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 16\u001b[0m\n\u001b[0;32m      6\u001b[0m     model\u001b[38;5;241m.\u001b[39mexport(\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m         imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m416\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m         batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     14\u001b[0m     )\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Export complete. Copy the .onnx file to your Jetson for TensorRT conversion.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_for_jetson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mruns/train/jetbot_groundview/weights/best.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 109\u001b[0m, in \u001b[0;36mJetsonOptimizedTrainer.optimize_for_jetson\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m     98\u001b[0m model\u001b[38;5;241m.\u001b[39mexport(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    100\u001b[0m     imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m416\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m     batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Single image inference\u001b[39;00m\n\u001b[0;32m    106\u001b[0m )\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# 2. Export to TensorRT for Jetson\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mengine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m416\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhalf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 4GB workspace\u001b[39;49;00m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel optimized for Jetson deployment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\model.py:733\u001b[0m, in \u001b[0;36mModel.export\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m custom \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    726\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgsz\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgsz\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    727\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    731\u001b[0m }  \u001b[38;5;66;03m# method defaults\u001b[39;00m\n\u001b[0;32m    732\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport\u001b[39m\u001b[38;5;124m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExporter\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\exporter.py:458\u001b[0m, in \u001b[0;36mExporter.__call__\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    456\u001b[0m     f[\u001b[38;5;241m0\u001b[39m], _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_torchscript()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine:  \u001b[38;5;66;03m# TensorRT required before ONNX\u001b[39;00m\n\u001b[1;32m--> 458\u001b[0m     f[\u001b[38;5;241m1\u001b[39m], _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdla\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdla\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m onnx:  \u001b[38;5;66;03m# ONNX\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     f[\u001b[38;5;241m2\u001b[39m], _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_onnx()\n",
      "File \u001b[1;32mc:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\exporter.py:197\u001b[0m, in \u001b[0;36mtry_export.<locals>.outer_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    196\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m export failure \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;241m.\u001b[39mt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\exporter.py:192\u001b[0m, in \u001b[0;36mtry_export.<locals>.outer_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Profile() \u001b[38;5;28;01mas\u001b[39;00m dt:\n\u001b[1;32m--> 192\u001b[0m         f, model \u001b[38;5;241m=\u001b[39m inner_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m export success ✅ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;241m.\u001b[39mt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms, saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_size(f)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f, model\n",
      "File \u001b[1;32mc:\\Users\\fredr\\anaconda3\\lib\\site-packages\\ultralytics\\engine\\exporter.py:888\u001b[0m, in \u001b[0;36mExporter.export_engine\u001b[1;34m(self, dla, prefix)\u001b[0m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m LINUX:\n\u001b[0;32m    887\u001b[0m         check_requirements(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorrt>7.0.0,!=10.1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 888\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorrt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtrt\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    889\u001b[0m check_version(trt\u001b[38;5;241m.\u001b[39m__version__, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=7.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, hard\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    890\u001b[0m check_version(trt\u001b[38;5;241m.\u001b[39m__version__, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!=10.1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/ultralytics/ultralytics/pull/14239\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorrt'"
     ]
    }
   ],
   "source": [
    "def optimize_for_jetson(self, model_path):\n",
    "    \"\"\"Export to ONNX format on PC. TensorRT conversion must be done on Jetson.\"\"\"\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    print(\"🔄 Exporting model to ONNX...\")\n",
    "    model.export(\n",
    "        format='onnx',\n",
    "        imgsz=512,\n",
    "        half=True,\n",
    "        simplify=True,\n",
    "        opset=12,\n",
    "        dynamic=False,\n",
    "        batch=1\n",
    "    )\n",
    "    print(\"✅ Export complete. Copy the .onnx file to your Jetson for TensorRT conversion.\")\n",
    "trainer.optimize_for_jetson('runs/train/jetbot_groundview/weights/best.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d27af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetbotGroundViewDetector:\n",
    "    \"\"\"\n",
    "    Optimized inference for Jetson Nano with ground-view perspective.\n",
    "    Handles arbitrary number of robots in frame.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, use_tensorrt=True):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        if use_tensorrt and model_path.endswith('.engine'):\n",
    "            # Use TensorRT for maximum performance\n",
    "            import tensorrt as trt\n",
    "            import pycuda.driver as cuda\n",
    "            import pycuda.autoinit\n",
    "            \n",
    "            self.trt_runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))\n",
    "            with open(model_path, 'rb') as f:\n",
    "                self.engine = self.trt_runtime.deserialize_cuda_engine(f.read())\n",
    "            self.context = self.engine.create_execution_context()\n",
    "            \n",
    "            # Allocate buffers\n",
    "            self.inputs, self.outputs, self.bindings = [], [], []\n",
    "            for binding in self.engine:\n",
    "                size = trt.volume(self.engine.get_binding_shape(binding))\n",
    "                dtype = trt.nptype(self.engine.get_binding_dtype(binding))\n",
    "                host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "                device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "                self.bindings.append(int(device_mem))\n",
    "                if self.engine.binding_is_input(binding):\n",
    "                    self.inputs.append({'host': host_mem, 'device': device_mem})\n",
    "                else:\n",
    "                    self.outputs.append({'host': host_mem, 'device': device_mem})\n",
    "        else:\n",
    "            # Use ONNX or PyTorch\n",
    "            self.model = YOLO(model_path)\n",
    "    \n",
    "    def preprocess_groundview(self, image):\n",
    "        \"\"\"Preprocess image from ground-view camera\"\"\"\n",
    "        # Resize to model input size\n",
    "        img = cv2.resize(image, (512, 512))\n",
    "        \n",
    "        # Convert to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Normalize\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Add batch dimension and transpose to NCHW\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img = np.expand_dims(img, 0)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def detect_robots_and_goal(self, image, conf_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Detect all robots and goal box with orientations.\n",
    "        Returns list of detections, each containing:\n",
    "        - class: 'robot' or 'goal_box'\n",
    "        - bbox: [x1, y1, x2, y2]\n",
    "        - confidence: detection confidence\n",
    "        - yaw: orientation in radians (robots only)\n",
    "        - distance: estimated distance based on bbox size\n",
    "        \"\"\"\n",
    "        \n",
    "        # Preprocess\n",
    "        input_img = self.preprocess_groundview(image)\n",
    "        \n",
    "        if hasattr(self, 'trt_runtime'):\n",
    "            # TensorRT inference\n",
    "            np.copyto(self.inputs[0]['host'], input_img.ravel())\n",
    "            cuda.memcpy_htod(self.inputs[0]['device'], self.inputs[0]['host'])\n",
    "            self.context.execute_v2(bindings=self.bindings)\n",
    "            cuda.memcpy_dtoh(self.outputs[0]['host'], self.outputs[0]['device'])\n",
    "            outputs = self.outputs[0]['host'].reshape(self.engine.get_binding_shape(1))\n",
    "        else:\n",
    "            # ONNX/PyTorch inference\n",
    "            results = self.model(input_img, conf=conf_threshold)\n",
    "            outputs = results[0].boxes\n",
    "        \n",
    "        # Parse detections\n",
    "        detections = []\n",
    "        \n",
    "        # Process each detection\n",
    "        if outputs is not None:\n",
    "            for det in outputs.data:\n",
    "                x1, y1, x2, y2, conf, cls = det[:6]\n",
    "                \n",
    "                # Get orientation if robot\n",
    "                if int(cls) == 0:  # Robot class\n",
    "                    sin_yaw = float(det[6])\n",
    "                    cos_yaw = float(det[7])\n",
    "                    yaw = np.arctan2(sin_yaw, cos_yaw)\n",
    "                else:\n",
    "                    yaw = None\n",
    "                \n",
    "                # Estimate distance from ground view\n",
    "                bbox_height = y2 - y1\n",
    "                # Simple distance estimation (calibrate for your camera)\n",
    "                distance = 1.0 / (bbox_height / 512.0 + 0.01)\n",
    "                \n",
    "                detection = {\n",
    "                    'class': 'robot' if int(cls) == 0 else 'goal_box',\n",
    "                    'bbox': [float(x1), float(y1), float(x2), float(y2)],\n",
    "                    'confidence': float(conf),\n",
    "                    'yaw': yaw,\n",
    "                    'distance': distance,\n",
    "                    'center_x': float((x1 + x2) / 2),\n",
    "                    'center_y': float((y1 + y2) / 2)\n",
    "                }\n",
    "                detections.append(detection)\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def compute_navigation_command(self, detections, fov=160):\n",
    "        \"\"\"\n",
    "        Compute navigation command based on ground-view detections.\n",
    "        Handles multiple robots and finds path to goal.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Find goal\n",
    "        goal = None\n",
    "        robots = []\n",
    "        \n",
    "        for det in detections:\n",
    "            if det['class'] == 'goal_box':\n",
    "                goal = det\n",
    "            else:\n",
    "                robots.append(det)\n",
    "        \n",
    "        if goal is None:\n",
    "            # No goal visible - search by rotating\n",
    "            return {\n",
    "                'linear_velocity': 0.0,\n",
    "                'angular_velocity': 0.5,  # Rotate to search\n",
    "                'status': 'searching_for_goal'\n",
    "            }\n",
    "        \n",
    "        # Compute angle to goal\n",
    "        img_center_x = 256  # 512 / 2\n",
    "        goal_pixel_offset = goal['center_x'] - img_center_x\n",
    "        goal_angle = (goal_pixel_offset / img_center_x) * (fov / 2) * np.pi / 180\n",
    "        \n",
    "        # Check for obstacles (other robots)\n",
    "        clear_path = True\n",
    "        min_safe_distance = 0.5  # meters\n",
    "        \n",
    "        for robot in robots:\n",
    "            # Check if robot is in our path\n",
    "            robot_angle = ((robot['center_x'] - img_center_x) / img_center_x) * (fov / 2) * np.pi / 180\n",
    "            \n",
    "            if abs(robot_angle) < 0.3 and robot['distance'] < min_safe_distance:\n",
    "                clear_path = False\n",
    "                break\n",
    "        \n",
    "        # Compute velocities\n",
    "        if clear_path:\n",
    "            # Proportional control toward goal\n",
    "            angular_vel = -2.0 * goal_angle  # Negative because image x is reversed\n",
    "            linear_vel = 0.5 * (1.0 - min(abs(goal_angle), 1.0))\n",
    "            status = 'moving_to_goal'\n",
    "        else:\n",
    "            # Obstacle avoidance\n",
    "            angular_vel = 0.8 if goal_angle > 0 else -0.8\n",
    "            linear_vel = 0.1\n",
    "            status = 'avoiding_obstacle'\n",
    "        \n",
    "        return {\n",
    "            'linear_velocity': float(linear_vel),\n",
    "            'angular_velocity': float(angular_vel),\n",
    "            'status': status,\n",
    "            'goal_distance': goal['distance'],\n",
    "            'num_robots_detected': len(robots)\n",
    "        }\n",
    "    \n",
    "    def visualize_detections(self, image, detections):\n",
    "        \"\"\"Visualize detections with ground-view perspective indicators\"\"\"\n",
    "        vis_img = image.copy()\n",
    "        \n",
    "        for det in detections:\n",
    "            x1, y1, x2, y2 = map(int, det['bbox'])\n",
    "            \n",
    "            # Color based on class\n",
    "            color = (0, 255, 0) if det['class'] == 'robot' else (0, 0, 255)\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(vis_img, (x1, y1), (x2, y2), color, 2)\n",
    "            \n",
    "            # Label with distance\n",
    "            label = f\"{det['class']} {det['confidence']:.2f} ({det['distance']:.1f}m)\"\n",
    "            cv2.putText(vis_img, label, (x1, y1-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "            \n",
    "            # Draw orientation arrow for robots\n",
    "            if det['yaw'] is not None:\n",
    "                cx, cy = int(det['center_x']), int(det['center_y'])\n",
    "                length = int((x2 - x1) * 0.4)\n",
    "                \n",
    "                # Arrow points in robot's facing direction\n",
    "                end_x = int(cx + length * np.cos(det['yaw']))\n",
    "                end_y = int(cy + length * np.sin(det['yaw']))\n",
    "                \n",
    "                cv2.arrowedLine(vis_img, (cx, cy), (end_x, end_y),\n",
    "                               (255, 255, 0), 2, tipLength=0.3)\n",
    "        \n",
    "        # Draw ground perspective grid\n",
    "        h, w = vis_img.shape[:2]\n",
    "        for y in range(h//4, h, h//8):\n",
    "            cv2.line(vis_img, (0, y), (w, y), (128, 128, 128), 1)\n",
    "        \n",
    "        return vis_img\n",
    "\n",
    "# Example usage on Jetson\n",
    "detector = JetbotGroundViewDetector('runs/train/jetbot_groundview/weights/best.engine')\n",
    "\n",
    "# Process frame from camera\n",
    "cap = cv2.VideoCapture(0)  # Or gstreamer pipeline for CSI camera\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Detect\n",
    "detections = detector.detect_robots_and_goal(frame)\n",
    "\n",
    "# Navigate\n",
    "nav_cmd = detector.compute_navigation_command(detections)\n",
    "print(f\"Navigation: Linear={nav_cmd['linear_velocity']:.2f}, \"\n",
    "      f\"Angular={nav_cmd['angular_velocity']:.2f}, \"\n",
    "      f\"Status={nav_cmd['status']}\")\n",
    "\n",
    "# Visualize\n",
    "vis_frame = detector.visualize_detections(frame, detections)\n",
    "cv2.imshow('Jetbot Ground View', vis_frame)\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a699f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_on_jetson(model_path, num_iterations=100):\n",
    "    \"\"\"Benchmark inference performance on Jetson Nano\"\"\"\n",
    "    import time\n",
    "    \n",
    "    detector = JetbotGroundViewDetector(model_path)\n",
    "    \n",
    "    # Warm up\n",
    "    dummy_img = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)\n",
    "    for _ in range(10):\n",
    "        _ = detector.detect_robots_and_goal(dummy_img)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(num_iterations):\n",
    "        start = time.time()\n",
    "        _ = detector.detect_robots_and_goal(dummy_img)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000  # ms\n",
    "    fps = 1000 / avg_time\n",
    "    \n",
    "    print(f\"Inference Performance on Jetson Nano:\")\n",
    "    print(f\"Average inference time: {avg_time:.2f} ms\")\n",
    "    print(f\"FPS: {fps:.1f}\")\n",
    "    print(f\"Min time: {np.min(times)*1000:.2f} ms\")\n",
    "    print(f\"Max time: {np.max(times)*1000:.2f} ms\")\n",
    "    \n",
    "    return fps\n",
    "\n",
    "# Benchmark different formats\n",
    "print(\"PyTorch model:\")\n",
    "benchmark_on_jetson('runs/train/jetbot_groundview/weights/best.pt')\n",
    "\n",
    "print(\"\\nONNX model:\")\n",
    "benchmark_on_jetson('runs/train/jetbot_groundview/weights/best.onnx')\n",
    "\n",
    "print(\"\\nTensorRT model:\")\n",
    "benchmark_on_jetson('runs/train/jetbot_groundview/weights/best.engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c58c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROS integration example (save as jetbot_detector_node.py)\n",
    "\"\"\"\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import rospy\n",
    "from sensor_msgs.msg import Image, CompressedImage\n",
    "from geometry_msgs.msg import Twist\n",
    "from std_msgs.msg import String\n",
    "from cv_bridge import CvBridge\n",
    "import json\n",
    "\n",
    "class JetbotDetectorNode:\n",
    "    def __init__(self):\n",
    "        rospy.init_node('jetbot_ground_detector')\n",
    "        \n",
    "        # Initialize detector\n",
    "        model_path = rospy.get_param('~model_path', 'best.engine')\n",
    "        self.detector = JetbotGroundViewDetector(model_path)\n",
    "        \n",
    "        # ROS setup\n",
    "        self.bridge = CvBridge()\n",
    "        self.image_sub = rospy.Subscriber('/camera/image_raw/compressed', \n",
    "                                         CompressedImage, \n",
    "                                         self.image_callback)\n",
    "        self.cmd_pub = rospy.Publisher('/cmd_vel', Twist, queue_size=1)\n",
    "        self.detection_pub = rospy.Publisher('/detections', String, queue_size=1)\n",
    "        \n",
    "        rospy.loginfo(\"Jetbot detector node started\")\n",
    "        \n",
    "    def image_callback(self, msg):\n",
    "        # Convert compressed image to OpenCV\n",
    "        np_arr = np.frombuffer(msg.data, np.uint8)\n",
    "        cv_image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        # Run detection\n",
    "        detections = self.detector.detect_robots_and_goal(cv_image)\n",
    "        \n",
    "        # Publish detections\n",
    "        det_msg = String()\n",
    "        det_msg.data = json.dumps(detections)\n",
    "        self.detection_pub.publish(det_msg)\n",
    "        \n",
    "        # Compute and publish navigation command\n",
    "        nav_cmd = self.detector.compute_navigation_command(detections)\n",
    "        \n",
    "        twist = Twist()\n",
    "        twist.linear.x = nav_cmd['linear_velocity']\n",
    "        twist.angular.z = nav_cmd['angular_velocity']\n",
    "        self.cmd_pub.publish(twist)\n",
    "        \n",
    "        rospy.loginfo(f\"Detected {len(detections)} objects, \"\n",
    "                     f\"Status: {nav_cmd['status']}\")\n",
    "    \n",
    "    def run(self):\n",
    "        rospy.spin()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        node = JetbotDetectorNode()\n",
    "        node.run()\n",
    "    except rospy.ROSInterruptException:\n",
    "        pass\n",
    "\"\"\" detector node started\")\n",
    "        \n",
    "    def image_callback(self, msg):\n",
    "        # Convert compressed image to OpenCV\n",
    "        np_arr = np.frombuffer(msg.data, np.uint8)\n",
    "        cv_image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        # Run detection\n",
    "        detections = self.detector.detect_robots_and_goal(cv_image)\n",
    "        \n",
    "        # Publish detections\n",
    "        det_msg = String()\n",
    "        det_msg.data = json.dumps(detections)\n",
    "        self.detection_pub.publish(det_msg)\n",
    "        \n",
    "        # Compute and publish navigation command\n",
    "        nav_cmd = self.detector.compute_navigation_command(detections)\n",
    "        \n",
    "        twist = Twist()\n",
    "        twist.linear.x = nav_cmd['linear_velocity']\n",
    "        twist.angular.z = nav_cmd['angular_velocity']\n",
    "        self.cmd_pub.publish(twist)\n",
    "        \n",
    "        rospy.loginfo(f\"Detected {len(detections)} objects, \"\n",
    "                     f\"Status: {nav_cmd['status']}\")\n",
    "    \n",
    "    def run(self):\n",
    "        rospy.spin()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        node = JetbotDetectorNode()\n",
    "        node.run()\n",
    "    except rospy.ROSInterruptException:\n",
    "        pass\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
